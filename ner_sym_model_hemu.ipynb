{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4ivrgieHRgD"
      },
      "source": [
        "## NER-SYM(Named Entity recognition using symbolic NLP)\n",
        "\n",
        "                                                                        (By :- Sai Hemanth Kilaru)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uh0cckAXmxeh"
      },
      "source": [
        "Note: I have tried my best and I have completed the project. i have referred to multiple web sources like stack overflow ,etc.. Here are the few links of the sources i have referred to:\n",
        "1. for regex:  'https://docs.python.org/3/library/re.html'\n",
        "2. for nltk and spacy: 'https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da\n",
        "'\n",
        "3. for evaluation metrics: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html   and my ML notebook.\n",
        "\n",
        "4. Assistant professor Kadir Bulut- helped me in solving few errors in the  code part.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiJGdxwZHRgF"
      },
      "source": [
        "#### Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4ynHqifHRgG",
        "outputId": "0ea4f428-2aa3-4192-87e5-ef53b8c4aefb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import spacy\n",
        "import re\n",
        "\n",
        "#we download the punkt and average perceptron trigger - used for tokenization and part of speech tagging,maxnet_ne_chunker-chunking process\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "# we download SpaCy's English model \n",
        "spacy.cli.download(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIyXbcx2HRgI"
      },
      "source": [
        "#### Loading the dataset\n",
        "\n",
        "\n",
        "we can get the WikiANN dataset  from the hugging face,directly by loading it from the datasets library.As here are multiple languages in the dataset, we import the one related to english 'en'.\n",
        "original dataset link: https://huggingface.co/datasets/unimelb-nlp/wikiann"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frPQtMZXHRgJ",
        "outputId": "fea3f1a6-4875-4c69-e87e-1edfb0fb08d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# loading the  WikiANN dataset (English) from the hugging face with the help of datasets library.\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"wikiann\", \"en\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ft4LNbN2HRgJ",
        "outputId": "a2205e10-7517-488f-b8fb-1312378d4684"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df_train\",\n  \"rows\": 20000,\n  \"fields\": [\n    {\n      \"column\": \"tokens\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ner_tags\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"langs\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"spans\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df_train"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-726ce3a0-08fa-4183-9e6f-87fd2a68218c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tokens</th>\n",
              "      <th>ner_tags</th>\n",
              "      <th>langs</th>\n",
              "      <th>spans</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[R.H., Saunders, (, St., Lawrence, River, ), (...</td>\n",
              "      <td>[3, 4, 0, 3, 4, 4, 0, 0, 0, 0, 0]</td>\n",
              "      <td>[en, en, en, en, en, en, en, en, en, en, en]</td>\n",
              "      <td>[ORG: R.H. Saunders, ORG: St. Lawrence River]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[;, ', '', Anders, Lindström, '', ']</td>\n",
              "      <td>[0, 0, 0, 1, 2, 0, 0]</td>\n",
              "      <td>[en, en, en, en, en, en, en]</td>\n",
              "      <td>[PER: Anders Lindström]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[Karl, Ove, Knausgård, (, born, 1968, )]</td>\n",
              "      <td>[1, 2, 2, 0, 0, 0, 0]</td>\n",
              "      <td>[en, en, en, en, en, en, en]</td>\n",
              "      <td>[PER: Karl Ove Knausgård]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[Atlantic, City, ,, New, Jersey]</td>\n",
              "      <td>[5, 6, 6, 6, 6]</td>\n",
              "      <td>[en, en, en, en, en]</td>\n",
              "      <td>[LOC: Atlantic City , New Jersey]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[Her, daughter, from, the, second, marriage, w...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[en, en, en, en, en, en, en, en, en, en, en, e...</td>\n",
              "      <td>[PER: Marie d'Agoult, PER: Franz Liszt, PER: C...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-726ce3a0-08fa-4183-9e6f-87fd2a68218c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-726ce3a0-08fa-4183-9e6f-87fd2a68218c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-726ce3a0-08fa-4183-9e6f-87fd2a68218c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3a695fa0-8d94-4471-926e-4a5542b97f61\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3a695fa0-8d94-4471-926e-4a5542b97f61')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3a695fa0-8d94-4471-926e-4a5542b97f61 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                              tokens  \\\n",
              "0  [R.H., Saunders, (, St., Lawrence, River, ), (...   \n",
              "1               [;, ', '', Anders, Lindström, '', ']   \n",
              "2           [Karl, Ove, Knausgård, (, born, 1968, )]   \n",
              "3                   [Atlantic, City, ,, New, Jersey]   \n",
              "4  [Her, daughter, from, the, second, marriage, w...   \n",
              "\n",
              "                                            ner_tags  \\\n",
              "0                  [3, 4, 0, 3, 4, 4, 0, 0, 0, 0, 0]   \n",
              "1                              [0, 0, 0, 1, 2, 0, 0]   \n",
              "2                              [1, 2, 2, 0, 0, 0, 0]   \n",
              "3                                    [5, 6, 6, 6, 6]   \n",
              "4  [0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, ...   \n",
              "\n",
              "                                               langs  \\\n",
              "0       [en, en, en, en, en, en, en, en, en, en, en]   \n",
              "1                       [en, en, en, en, en, en, en]   \n",
              "2                       [en, en, en, en, en, en, en]   \n",
              "3                               [en, en, en, en, en]   \n",
              "4  [en, en, en, en, en, en, en, en, en, en, en, e...   \n",
              "\n",
              "                                               spans  \n",
              "0      [ORG: R.H. Saunders, ORG: St. Lawrence River]  \n",
              "1                            [PER: Anders Lindström]  \n",
              "2                          [PER: Karl Ove Knausgård]  \n",
              "3                  [LOC: Atlantic City , New Jersey]  \n",
              "4  [PER: Marie d'Agoult, PER: Franz Liszt, PER: C...  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Just to look at the dataset, i have used pandas here. tbh,there's no use of it here other than this.\n",
        "import pandas as pd\n",
        "#converting it into a pandas dataframe\n",
        "df_train = pd.DataFrame(dataset[\"train\"])\n",
        "#first 5 rows of the dataset\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDnQYy5gMkLk",
        "outputId": "a18bc5b2-beb2-47ad-d818-6e1b88add716"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(20000, 4)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#shape\n",
        "df_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jrD3VnB3OT_",
        "outputId": "1c410e5a-978b-410a-e29c-472f6a10c181"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 20000 entries, 0 to 19999\n",
            "Data columns (total 4 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   tokens    20000 non-null  object\n",
            " 1   ner_tags  20000 non-null  object\n",
            " 2   langs     20000 non-null  object\n",
            " 3   spans     20000 non-null  object\n",
            "dtypes: object(4)\n",
            "memory usage: 625.1+ KB\n"
          ]
        }
      ],
      "source": [
        "#checking the stats\n",
        "df_train.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RFn6i-tmHRgJ"
      },
      "outputs": [],
      "source": [
        "#load the spacy's english model which we have downloaded earlier\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx3zxjHeHRgJ"
      },
      "source": [
        "#### Data Preprocessing\n",
        "Our Symbolic NER system focusses on extracting  the named entities from the WikiANN dataset.So,we should avoid applying stemming or lemmatization such that it preserve the integrity and enhances the accuracy of NER results.(Based on my research(I have tried to use it on the earlier code chunks,I found the current approach better))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "By2vGGVNHRgK"
      },
      "outputs": [],
      "source": [
        "\n",
        "from nltk import pos_tag, ne_chunk\n",
        "from spacy import displacy\n",
        "#We create a preprocess function which will tokenize the text, apply POS tagging, and use chunking(for multi-word entities)\n",
        "def preprocess(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "    chunks = nltk.ne_chunk(pos_tags)\n",
        "    return tokens, pos_tags, chunks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kAAjyC6HRgK"
      },
      "source": [
        "##### Entity Extraction functions\n",
        "\n",
        "Here,the entities are: Names, Dates, Locations, and Organizations. we use different functions witht the help of chunking and regex to extract these from the data.\n",
        "\n",
        "For the regex part down there , i have learnt regular expressions last year through https://www.youtube.com/watch?v=vsa9GGzMFXQ . This lecture is intermediate-level and contained the few necessary topics i need.i revised it once again before doing this ; which helped me with the entity extraction process in writing the few functions. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9AwRvn9aHRgK"
      },
      "outputs": [],
      "source": [
        "#function to identify the person names(chunking is used). I have added appending the remaining name part; in order to retreive the full name\n",
        "def extract_names(chunks):\n",
        "    names = []\n",
        "    current_name = []\n",
        "\n",
        "    for chunk in chunks:\n",
        "        # Check if the chunk is a named entity of type 'PERSON'\n",
        "        if hasattr(chunk, 'label') and chunk.label() == 'PERSON':\n",
        "            current_name.append(\" \".join(c[0] for c in chunk))\n",
        "        else:\n",
        "            if current_name:\n",
        "                # Append the  name accumulated\n",
        "                names.append(\" \".join(current_name))\n",
        "                current_name = []  # reset- for the next potential name\n",
        "\n",
        "    if current_name:  # append other name, it goes on....\n",
        "        names.append(\" \".join(current_name))\n",
        "\n",
        "    return names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzzMo8JdpsuT",
        "outputId": "b9b1b0e5-a4d7-4078-a183-e3402360821b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: Marie Curie was born in 19 May 1867 and worked in Paris in  university of az.\n",
            "Extracted Names: ['Marie Curie']\n",
            "\n",
            "Text: Albert Einstein developed the theory of relativity in Germany, 1947 in office.\n",
            "Extracted Names: ['Albert Einstein']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#testing my function with 2 sample strings\n",
        "# consider two sample strings(taken from the web and added few more words to it )\n",
        "sample_text_1 = \"Marie Curie was born in 19 May 1867 and worked in Paris in  university of az.\"\n",
        "sample_text_2 = \"Albert Einstein developed the theory of relativity in Germany, 1947 in office.\"\n",
        "\n",
        "# Testing Name Extraction, we already have a preprocessing function above, just for the checking sake , i have done all the operations again\n",
        "tokens_1 = nltk.word_tokenize(sample_text_1)\n",
        "chunks_1 = ne_chunk(nltk.pos_tag(tokens_1))\n",
        "print(f\"Text: {sample_text_1}\")\n",
        "print(f\"Extracted Names: {extract_names(chunks_1)}\\n\")\n",
        "\n",
        "tokens_2 = nltk.word_tokenize(sample_text_2)\n",
        "chunks_2 = ne_chunk(nltk.pos_tag(tokens_2))\n",
        "print(f\"Text: {sample_text_2}\")\n",
        "print(f\"Extracted Names: {extract_names(chunks_2)}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "76p9JJFpHRgK"
      },
      "outputs": [],
      "source": [
        "#function to identify the dates.(with the help of regex date pattern) [Took help from the professor 'bulut'] and inspired from the above source. \n",
        "#Here, at starting , i have thought of writing a regex code only in the MM-DD-YYYY format . but i  have observed that most of the texts consists of year , date and month seperately, so i have thought of going for the seperate functions.\n",
        "\n",
        "import re\n",
        "def extract_dates(text):\n",
        "    # extracting the years\n",
        "    year_pattern = r'\\b(?:18|19|20)\\d{2}\\b'  # matches the years\n",
        "    years = re.findall(year_pattern, text)\n",
        "\n",
        "    # extracting months in both the formats(example:Jan, January)\n",
        "    month_pattern = r'\\b(January|February|March|April|May|June|July|August|September|October|November|December|' \\\n",
        "                    r'Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\b'\n",
        "    months = re.findall(month_pattern, text)\n",
        "\n",
        "    # extracting the  day/s\n",
        "    day_pattern = r'\\b([1-9]|[12][0-9]|3[01])(st|nd|rd|th)?\\b'\n",
        "    days = re.findall(day_pattern, text)\n",
        "    days = [day[0] for day in days]  # Extract the day numbers from the tuple\n",
        "\n",
        "    # we ll return components that are found in the sentence,if not then none\n",
        "    date_components = {\n",
        "        'Years': years if years else None,\n",
        "        'Months': months if months else None,\n",
        "        'Days': days if days else None\n",
        "    }\n",
        "\n",
        "    # Filtering  out the None values and return\n",
        "    return {k: v for k, v in date_components.items() if v}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RnhwU6Lpzoj",
        "outputId": "dcc3a0e5-575e-4e4a-99eb-ed8dfbe586ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: Marie Curie was born in 19 May 1867 and worked in Paris in  university of az.\n",
            "Extracted Dates: {'Years': ['1867'], 'Months': ['May'], 'Days': ['19']}\n",
            "\n",
            "Text: Albert Einstein developed the theory of relativity in Germany, 1947 in office.\n",
            "Extracted Dates: {'Years': ['1947']}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Testing the extraction date function\n",
        "print(f\"Text: {sample_text_1}\")\n",
        "print(f\"Extracted Dates: {extract_dates(sample_text_1)}\\n\")\n",
        "\n",
        "print(f\"Text: {sample_text_2}\")\n",
        "print(f\"Extracted Dates: {extract_dates(sample_text_2)}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dQgL9X_3HRgK"
      },
      "outputs": [],
      "source": [
        "#function to identify the locations(chunking is used ,we check proper nouns/prepositions indicating the locations)(similar style to name function)\n",
        "def extract_locations(chunks, tagged_tokens):\n",
        "    locations = []\n",
        "    temp_location = []\n",
        "\n",
        "    for chunk in chunks:\n",
        "        # Check if the chunk is a named entity of type GPE (Geopolitical Entity)\n",
        "        if hasattr(chunk, 'label') and chunk.label() == 'GPE':\n",
        "            temp_location.append(\" \".join(c[0] for c in chunk))\n",
        "        elif temp_location:\n",
        "            # append that location\n",
        "            locations.append(\" \".join(temp_location))\n",
        "            temp_location = []  # reset- for the next potential location\n",
        "\n",
        "    if temp_location:  # append  remaining locations\n",
        "        locations.append(\" \".join(temp_location))\n",
        "\n",
        "    return locations\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwydUrx0p-fr",
        "outputId": "fb244667-84d5-41d9-a640-d5bb2a77d73a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: Marie Curie was born in 19 May 1867 and worked in Paris in  university of az.\n",
            "Extracted Locations: ['Paris']\n",
            "\n",
            "Text: Albert Einstein developed the theory of relativity in Germany, 1947 in office.\n",
            "Extracted Locations: ['Germany']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#testing extract location function for the above sample text\n",
        "print(f\"Text: {sample_text_1}\")\n",
        "print(f\"Extracted Locations: {extract_locations(chunks_1, nltk.pos_tag(tokens_1))}\\n\")\n",
        "\n",
        "print(f\"Text: {sample_text_2}\")\n",
        "print(f\"Extracted Locations: {extract_locations(chunks_2, nltk.pos_tag(tokens_2))}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9dky5oD2HRgL"
      },
      "outputs": [],
      "source": [
        "#function to identify the organization.(with the help of spacy's named entity recognition(as per my research,this is the simplier way to do it(without that, it gave a very less accuracy earlier))(source: already mentioned above )\n",
        "\n",
        "def extract_organizations(text):\n",
        "    doc = nlp(text)  # Processing the text with SpaCy\n",
        "    organizations = [ent.text for ent in doc.ents if ent.label_ == \"ORG\"]  # Extraction of organizations\n",
        "    return organizations\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVoYh2t9qJ89",
        "outputId": "73d831b0-7dfd-4ee7-dfc9-16ce66f8c58e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: Marie Curie worked with University of Paris.\n",
            "Extracted Organizations: ['University of Paris']\n",
            "\n",
            "Text: He is a scientist at Google.\n",
            "Extracted Organizations: ['Google']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample strings to check this(took from web and modified as it contains te org name)\n",
        "sample_text_3 = \"Marie Curie worked with University of Paris.\"\n",
        "sample_text_4 = \"He is a scientist at Google.\"\n",
        "\n",
        "# Testing Organization Extraction\n",
        "print(f\"Text: {sample_text_3}\")\n",
        "print(f\"Extracted Organizations: {extract_organizations(sample_text_3)}\\n\")\n",
        "\n",
        "print(f\"Text: {sample_text_4}\")\n",
        "print(f\"Extracted Organizations: {extract_organizations(sample_text_4)}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiWW7VG9HRgL"
      },
      "source": [
        "##### Combining all these individual entities into Symbolic NER System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "FeTIHrqEHRgL"
      },
      "outputs": [],
      "source": [
        "def symbolic_ner(text):\n",
        "    tokens, pos_tags, chunks = preprocess(text)  # to preprocess text\n",
        "    names = extract_names(chunks)  # to extract names\n",
        "    dates = extract_dates(text)  # to extract dates\n",
        "    locations = extract_locations(chunks, pos_tags)  # to extract locations\n",
        "    organizations = extract_organizations(text)  # to extract organizations\n",
        "\n",
        "    # we ll create a dictionary where the keys are entity types and the values are extracted entities from the text\n",
        "    entities = {\n",
        "        'Names': names,\n",
        "        'Dates': dates,\n",
        "        'Locations': locations,\n",
        "        'Organizations': organizations\n",
        "    }\n",
        "\n",
        "    return entities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kIYyHMV9gqY"
      },
      "source": [
        "##### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "A0Tdhd1iHRgM"
      },
      "outputs": [],
      "source": [
        "# now,we ll write a function to test the system on the entire dataset and then call it\n",
        "def test_full_dataset(dataset):\n",
        "  #intialise the following:\n",
        "  #For the better model evaluation, i have decided to create a counter seperately for the correct and total entities\n",
        "    symbolic_correct_names = 0\n",
        "    symbolic_total_names = 0\n",
        "    symbolic_correct_dates = 0\n",
        "    symbolic_total_dates = 0\n",
        "    symbolic_correct_locations = 0\n",
        "    symbolic_total_locations = 0\n",
        "    symbolic_correct_organizations = 0\n",
        "    symbolic_total_organizations = 0\n",
        "  #create a empty list to store the detailed observations\n",
        "    detailed_observations = []\n",
        "\n",
        "    # iterate through all samples in the dataset\n",
        "    for sample in dataset[\"train\"]:  # through the entire dataset\n",
        "        sentence = \" \".join(sample[\"tokens\"])  # joining the  tokens to form a complete sentence\n",
        "        entities = symbolic_ner(sentence)  # getting the  entities\n",
        "\n",
        "        # extracting the true entities\n",
        "        true_names = extract_names(ne_chunk(nltk.pos_tag(sample[\"tokens\"])))# names\n",
        "        true_dates = extract_dates(sentence)# dates\n",
        "        true_locations = extract_locations(ne_chunk(nltk.pos_tag(sample[\"tokens\"])), nltk.pos_tag(sample[\"tokens\"]))#locations\n",
        "        true_organizations = extract_organizations(sentence)#organisations\n",
        "#now we ll update the total entities and ll take the count of the correct entities.\n",
        "        # evaluating the names(This whole process till the overall accuracy,the process has been inspired from a few comments on the stack overflow and also got a little help from the professor 'bulut')\n",
        "        symbolic_total_names += len(true_names)\n",
        "        symbolic_correct_names += len(set(true_names) & set(entities['Names']))\n",
        "\n",
        "        # evaluating the dates\n",
        "        symbolic_total_dates += len(true_dates)\n",
        "        symbolic_correct_dates += len(set(true_dates) & set(entities['Dates']))\n",
        "\n",
        "        # evaluating the locations\n",
        "        symbolic_total_locations += len(true_locations)\n",
        "        symbolic_correct_locations += len(set(true_locations) & set(entities['Locations']))\n",
        "\n",
        "        # evaluating the  organizations\n",
        "        symbolic_total_organizations += len(true_organizations)\n",
        "        symbolic_correct_organizations += len(set(true_organizations) & set(entities['Organizations']))\n",
        "\n",
        "        # we ll create detailed observation for this sample (for the better understanding)\n",
        "        observation = {\n",
        "            'Sentence': sentence,\n",
        "            'Entities Found': entities,\n",
        "            'True Names': true_names,\n",
        "            'Correctly Identified Names': set(true_names) & set(entities['Names']),\n",
        "            'True Dates': true_dates,\n",
        "            'Correctly Identified Dates': set(true_dates) & set(entities['Dates']),\n",
        "            'True Locations': true_locations,\n",
        "            'Correctly Identified Locations': set(true_locations) & set(entities['Locations']),\n",
        "            'True Organizations': true_organizations,\n",
        "            'Correctly Identified Organizations': set(true_organizations) & set(entities['Organizations'])\n",
        "        }\n",
        "        detailed_observations.append(observation)\n",
        "\n",
        "    # calculate overall accuracy for all entities, we divie these two , such that we get the exact accuracy for our entities()\n",
        "    symbolic_accuracy_names = symbolic_correct_names / symbolic_total_names if symbolic_total_names > 0 else 0\n",
        "    symbolic_accuracy_dates = symbolic_correct_dates / symbolic_total_dates if symbolic_total_dates > 0 else 0\n",
        "    symbolic_accuracy_locations = symbolic_correct_locations / symbolic_total_locations if symbolic_total_locations > 0 else 0\n",
        "    symbolic_accuracy_organizations = symbolic_correct_organizations / symbolic_total_organizations if symbolic_total_organizations > 0 else 0\n",
        "\n",
        "    # we ll print  overall accuracy for each entity type(for better understanding)\n",
        "    print(f\"\\nOverall Accuracy of Symbolic NER Model for Names: {symbolic_accuracy_names:.4f}\")\n",
        "    print(f\"Overall Accuracy of Symbolic NER Model for Dates: {symbolic_accuracy_dates:.4f}\")\n",
        "    print(f\"Overall Accuracy of Symbolic NER Model for Locations: {symbolic_accuracy_locations:.4f}\")\n",
        "    print(f\"Overall Accuracy of Symbolic NER Model for Organizations: {symbolic_accuracy_organizations:.4f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxVb7KBJbxhb",
        "outputId": "220b930e-8075-40fe-d24c-5212b9f5b287"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Overall Accuracy of Symbolic NER Model for Names: 0.9793\n",
            "Overall Accuracy of Symbolic NER Model for Dates: 1.0000\n",
            "Overall Accuracy of Symbolic NER Model for Locations: 0.9828\n",
            "Overall Accuracy of Symbolic NER Model for Organizations: 0.9984\n"
          ]
        }
      ],
      "source": [
        "# Call the function to test the full dataset\n",
        "test_full_dataset(dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHr2RMl0ZDYo"
      },
      "source": [
        "From the above code observations , we came to know that the symbolic nlp for the NER works well for the date,organization followed by the name and location(in very few cases among the 20000 samples). This name function's accuracy can be improved by adding the PERSON label of spacy's a;ong with the names function and GPE and LOC label of spacy's for the location function , this ll result in a higher accuracy. But, as my project is related to symbolic nlp specifically, i have erased those added codelines , as it seemed to reduce the symbolic nlp's significance in my Project. The date function worked well when we considered extracting the day,month,year instead of the DD/MM/YYYY format.\n",
        "\n",
        "ran the code in : google collab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## THANK YOU!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "JFTIkAWhHRgN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "bP1JQ0P_HRgN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ewlAJ_grHRgN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "PcHJXJRuHRgN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xKwtm6ULHRgN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "k0HkcBiPHRgN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
